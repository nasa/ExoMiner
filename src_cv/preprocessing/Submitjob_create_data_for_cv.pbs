# Conduct preprocessing run that creates a TFRecord dataset for CV; uses GNU parallel to generate multiple processes,
# each one conducting the preprocessing of a subset of the examples.
#PBS -S /bin/bash
#PBS -N preprocess_data_cv_exoplanet
#PBS -l walltime=72:00:00
# non-GPU nodes ----
# PBS -l select=1:ncpus=10:model=rom_ait
# PBS -q debug
# DSG V100 GPU nodes ----
#PBS -l select=1:ncpus=18:mem=180g:model=sky_gpu
#PBS -l place=free:shared
#PBS -q dsg_gpu@pbspl4
# GH nodes ----
# PBS -lselect=1:ncpus=72:ngpus=1:model=gh200
# PBS -q dsggh_gpu@pbs05a
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_preprocess_data_cv.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_preprocess_data_cv.err
#PBS -W group_list=a1509
# PBS -W group_list=s2857
#PBS -m bea
#PBS -J 0-4

# initialize conda and activate conda environment
module use -a /swbuild/analytix/tools/modulefiles
# GH nodes ---
# module load miniconda3/gh2
# source activate exoplnt_dl_gh_nfs
# other nodes ----
# source "$HOME"/.bashrc
module load miniconda3/v4
source activate exoplnt_dl_tf2_13

# set path to codebase root directory
export PYTHONPATH=/home6/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/

# create output directory for preprocessing results
OUTPUT_DIR=/u/msaragoc/work_dir/Kepler-TESS_exoplanet/data/tfrecords/TESS/cv_tfrecords_tess-spoc-tces_2min-s1-s94_ffi-s36-s72-s56s69_10-30-2025_1406/tfrecords/eval_normalized/
mkdir -p $OUTPUT_DIR

# create main output job file
JOB_FP=$OUTPUT_DIR/output_job"$PBS_ARRAY_INDEX".txt

exec >> "$JOB_FP" 2>&1

echo "Job started at $(date)" >> $JOB_FP
echo "PBS Job ID: $PBS_JOBID" >> $JOB_FP
echo "PBS Job array: $PBS_ARRAY_INDEX" >> $JOB_FP

# copy PBS script
# PBS_SCRIPT_FP=$(realpath $0)
PBS_SCRIPT_FP="$PBS_O_WORKDIR/$(basename $0)"
cp $PBS_SCRIPT_FP $JOB_FP

# copy codebase git commit hash
COMMIT_HASH=$(git -C $PYTHONPATH rev-parse HEAD)
echo "Git hash commit: $COMMIT_HASH"  >> $JOB_FP

# script file path
SCRIPT_FP=$PYTHONPATH/src_cv/preprocessing/preprocess_cv_folds_trecord_dataset.py
# config file path
CONFIG_FP=$PYTHONPATH/src_cv/preprocessing/config_preprocess_cv_folds_tfrecord_dataset.yaml
# job script for running preprocessing pipeline
PREPROCESS_SH_SCRIPT=$PYTHONPATH/src_cv/preprocessing/preprocessing_job.sh

START_TIME=$(date +%s)

# number of total jobs; CV iterations
NUM_TOTAL_JOBS=5

# run with GNU parallel
# number of jobs run simultaneously
# NUM_JOBS_PARALLEL=1
# seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL --sshloginfile "$PBS_NODEFILE" "$PREPROCESS_SH_SCRIPT {} $OUTPUT_DIR $SCRIPT_FP $CONFIG_FP $NUM_TOTAL_JOBS"

# run with JOB ARRAY
$PREPROCESS_SH_SCRIPT $PBS_ARRAY_INDEX $OUTPUT_DIR $SCRIPT_FP $CONFIG_FP $NUM_TOTAL_JOBS

echo "Job $JOB_FP finished at $(date)" >> $JOB_FP

END_TIME=$(date +%s)
echo "Duration: $((END_TIME - START_TIME)) seconds" >> $JOB_FP
