# Run train, eval experiment in parallel (multiple models run at the same time) on a single node with multiple GPUs and
# using a combination of GNU parallel and job array; option for multi-node setup.
#PBS -S /bin/bash
#PBS -N test_train_newexominer
#PBS -l walltime=72:00:00
# DSG_GPU V100 -------
#PBS -l select=1:ncpus=36:ngpus=1:mem=360g:model=sky_gpu
# place the chunk wherever it is possible for the requested resources; share resources with other people
#PBS -l place=free:shared
#PBS -q dsg_gpu@pbspl4
# GH --------
# PBS -lselect=1:ncpus=72:ngpus=1:model=gh200
# PBS -q dsggh_gpu@pbs05a
#PBS -o /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_test_train_newexominer.out
#PBS -e /home6/msaragoc/jobs/Kepler-TESS_exoplanet/job_test_train_newexominer.err
#PBS -W group_list=a1509
#PBS -m bea
# PBS -J 0-2

# initialize conda and activate conda environment
module use -a /swbuild/analytix/tools/modulefiles
# non-GH nodes
module load miniconda3/v4
source activate exoplnt_dl_tf2_13
# GH nodes
# module load miniconda3/gh2
# source activate exoplnt_dl_gh_nfs

# PBS_ARRAY_INDEX=0
# export TMPDIR=/var/tmp/pbs.34282.pbs05a.gh.nas.nasa.gov
# mkdir -p "$TMPDIR"

# path to codebase root directory
export PYTHONPATH=/nobackupp19/msaragoc/work_dir/Kepler-TESS_exoplanet/codebase/

# config file path
CONFIG_FP=$PYTHONPATH/src/train/config_train.yaml
# job script for running the Python application
RUN_SH_SCRIPT=$PYTHONPATH/src/train/run_train_iter.sh

# output directory
OUTPUT_DIR=/u/msaragoc/work_dir/Kepler-TESS_exoplanet/experiments/test_exominer_architectures/test_tess-spoc-2min-s1-s88_10-22-2025_1616/
mkdir -p $OUTPUT_DIR

N_MODELS=1  # total number of models to be trained

# number of GPUs to be used by this job array
N_GPUS_TOTAL=1

# number of total jobs per job in job array
NUM_TOTAL_JOBS=1  # $((1 * 4))
# number of jobs run simultaneously
NUM_JOBS_PARALLEL=1

# create main output job file
JOB_FP=$OUTPUT_DIR/output_job.txt

# copy PBS script
PBS_SCRIPT_FP=$(realpath $0)
cp $PBS_SCRIPT_FP $JOB_FP

# copy codebase git commit hash
COMMIT_HASH=$(git -C $PYTHONPATH rev-parse HEAD)
echo "Git hash commit: $COMMIT_HASH"  >> $JOB_FP

# run with GNU parallel
# seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL "$RUN_SH_SCRIPT {} $PBS_ARRAY_INDEX $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL $N_MODELS"
# testing multi-node setup...
# seq 0 $((NUM_TOTAL_JOBS - 1)) | parallel -j $NUM_JOBS_PARALLEL --sshloginfile $PBS_NODEFILE "$RUN_SH_SCRIPT {} 0 $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL $N_MODELS"

# create average ensemble model with trained models and evaluate ensemble
# EVAL_ENSEMBLE_SCRIPT=$PYTHONPATH/src/run_ensemble.sh
# $EVAL_ENSEMBLE_SCRIPT $OUTPUT_DIR $OUTPUT_DIR/model0/config_run.yaml $OUTPUT_DIR/ensemble_avg_model/

# run train, evaluation, and inference for a single model
$RUN_SH_SCRIPT 0 0 $CONFIG_FP $OUTPUT_DIR $N_GPUS_TOTAL $N_MODELS